# -*- coding: utf-8 -*-
"""
ä½ç«¯GPUæ¸²æŸ“å¼•æ“ç»¼åˆæµ‹è¯•ä¸éªŒè¯å·¥å…·
æ•´åˆæ€§èƒ½åŸºå‡†æµ‹è¯•å’Œè§†è§‰è´¨é‡éªŒè¯åŠŸèƒ½
"""

import os
import sys
import time
import json
import subprocess
import platform
from datetime import datetime
import numpy as np

# æ·»åŠ å¼•æ“æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# å¯¼å…¥å…¶ä»–æ¨¡å—
from Engine.Utils.Benchmark import Benchmark
from Engine.Utils.QualityValidator import QualityValidator
from Engine.Platform import PlatformInfo


class ComprehensiveTester:
    """ç»¼åˆæµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, engine=None):
        """åˆå§‹åŒ–ç»¼åˆæµ‹è¯•å·¥å…·
        
        Args:
            engine: æ¸²æŸ“å¼•æ“å®ä¾‹ï¼ˆå¯é€‰ï¼‰
        """
        self.engine = engine
        self.platform_info = PlatformInfo()
        self.benchmark = Benchmark(engine)
        self.quality_validator = QualityValidator(engine)
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            "performance_tests": {
                "enabled": True,
                "test_scenes": [
                    "standard_scene",
                    "high_detail_scene",
                    "low_light_scene",
                    "motion_scene"
                ],
                "duration_per_test": 30.0,  # ç§’
                "warmup_time": 5.0,        # ç§’
                "measure_memory": True,
                "measure_cpu": True
            },
            "quality_tests": {
                "enabled": True,
                "reference_images_dir": os.path.join("data", "reference_images"),
                "output_images_dir": os.path.join("data", "output_images"),
                "rtx_reference_dir": os.path.join("data", "rtx4090_reference"),
                "generate_comparison_images": True,
                "save_error_maps": True
            },
            "target_hardware": {
                "GTX_750Ti": {
                    "expected_fps": 30.0,
                    "max_vram_usage": 1800,  # MB
                    "pass_threshold": 0.8     # 80%çš„æµ‹è¯•é€šè¿‡
                },
                "RX_580": {
                    "expected_fps": 60.0,
                    "max_vram_usage": 3600,  # MB
                    "pass_threshold": 0.9     # 90%çš„æµ‹è¯•é€šè¿‡
                }
            },
            "output": {
                "reports_dir": os.path.join("data", "reports"),
                "save_json_report": True,
                "save_text_report": True,
                "generate_summary_image": False
            }
        }
        
        # æµ‹è¯•ç»“æœ
        self.results = {
            "test_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "platform_info": {},
            "performance_results": {},
            "quality_results": {},
            "summary": {
                "performance_passed": False,
                "quality_passed": False,
                "overall_passed": False,
                "total_tests": 0,
                "passed_tests": 0,
                "pass_rate": 0.0
            }
        }
        
        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self._create_directories()
    
    def _create_directories(self):
        """åˆ›å»ºæµ‹è¯•æ‰€éœ€çš„ç›®å½•ç»“æ„"""
        directories = [
            self.test_config["quality_tests"]["reference_images_dir"],
            self.test_config["quality_tests"]["output_images_dir"],
            self.test_config["quality_tests"]["rtx_reference_dir"],
            self.test_config["output"]["reports_dir"]
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
    
    def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•ï¼ˆæ€§èƒ½å’Œè´¨é‡ï¼‰
        
        Returns:
            dict: ç»¼åˆæµ‹è¯•ç»“æœ
        """
        print("\n========== å¼€å§‹ç»¼åˆæµ‹è¯• ==========\n")
        
        # è·å–å¹³å°ä¿¡æ¯
        self._collect_platform_info()
        
        # è¿è¡Œæ€§èƒ½æµ‹è¯•
        if self.test_config["performance_tests"]["enabled"]:
            self.run_performance_tests()
        
        # è¿è¡Œè´¨é‡æµ‹è¯•
        if self.test_config["quality_tests"]["enabled"]:
            self.run_quality_tests()
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        self._generate_summary()
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_reports()
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦
        self._display_summary()
        
        print("\n========== ç»¼åˆæµ‹è¯•å®Œæˆ ==========\n")
        return self.results
    
    def _collect_platform_info(self):
        """æ”¶é›†å¹³å°å’Œç¡¬ä»¶ä¿¡æ¯"""
        print("æ”¶é›†å¹³å°ä¿¡æ¯...")
        
        # ä½¿ç”¨PlatformInfoæ¨¡å—è·å–ä¿¡æ¯
        self.platform_info.initialize()
        
        # ä¿å­˜å¹³å°ä¿¡æ¯
        self.results["platform_info"] = {
            "os": self.platform_info.os_info,
            "gpu": self.platform_info.gpu_info,
            "cpu": self.platform_info.cpu_info,
            "system_memory": self.platform_info.system_memory,
            "gpu_memory": self.platform_info.vram_size,
            "directx_version": self.platform_info.directx_version,
            "opengl_version": self.platform_info.opengl_version
        }
        
        print(f"æµ‹è¯•å¹³å°: {self.platform_info.os_info}")
        print(f"GPU: {self.platform_info.gpu_info}")
        print(f"VRAM: {self.platform_info.vram_size} MB")
        print(f"CPU: {self.platform_info.cpu_info}")
        print(f"ç³»ç»Ÿå†…å­˜: {self.platform_info.system_memory} GB")
    
    def run_performance_tests(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n========== å¼€å§‹æ€§èƒ½æµ‹è¯• ==========")
        
        # åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•ç»“æœ
        self.results["performance_results"] = {
            "test_scenes": {},
            "average_fps": 0.0,
            "min_fps": float('inf'),
            "max_fps": 0.0,
            "average_vram_usage": 0.0,
            "peak_vram_usage": 0.0,
            "cpu_usage": 0.0,
            "passed": False
        }
        
        total_fps = 0.0
        scene_count = 0
        total_vram = 0.0
        
        # è¿è¡Œæ¯ä¸ªåœºæ™¯çš„æµ‹è¯•
        for scene_name in self.test_config["performance_tests"]["test_scenes"]:
            print(f"\næµ‹è¯•åœºæ™¯: {scene_name}")
            
            # è®¾ç½®åŸºå‡†æµ‹è¯•å‚æ•°
            self.benchmark.set_test_parameters(
                duration=self.test_config["performance_tests"]["duration_per_test"],
                warmup_time=self.test_config["performance_tests"]["warmup_time"],
                measure_memory=self.test_config["performance_tests"]["measure_memory"],
                measure_cpu=self.test_config["performance_tests"]["measure_cpu"]
            )
            
            # å¦‚æœæœ‰å¼•æ“å®ä¾‹ï¼ŒåŠ è½½åœºæ™¯
            if self.engine and hasattr(self.engine, 'load_scene'):
                try:
                    print(f"åŠ è½½åœºæ™¯ {scene_name}...")
                    # è¿™é‡Œå‡è®¾å¼•æ“æœ‰load_sceneæ–¹æ³•ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦è°ƒæ•´
                    self.engine.load_scene(scene_name)
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•åŠ è½½åœºæ™¯ {scene_name}: {e}")
                    # åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿçš„åœºæ™¯æµ‹è¯•ç»“æœ
                    scene_result = self._create_mock_scene_result(scene_name)
                else:
                    # è¿è¡ŒåŸºå‡†æµ‹è¯•
                    scene_result = self.benchmark.run_benchmark(scene_name)
            else:
                # æ²¡æœ‰å¼•æ“å®ä¾‹ï¼Œåˆ›å»ºæ¨¡æ‹Ÿç»“æœ
                scene_result = self._create_mock_scene_result(scene_name)
            
            # ä¿å­˜åœºæ™¯ç»“æœ
            self.results["performance_results"]["test_scenes"][scene_name] = scene_result
            
            # æ›´æ–°ç»Ÿè®¡æ•°æ®
            if scene_result["avg_fps"] > 0:
                total_fps += scene_result["avg_fps"]
                scene_count += 1
                self.results["performance_results"]["min_fps"] = min(
                    self.results["performance_results"]["min_fps"],
                    scene_result["min_fps"]
                )
                self.results["performance_results"]["max_fps"] = max(
                    self.results["performance_results"]["max_fps"],
                    scene_result["max_fps"]
                )
                
                if "avg_vram" in scene_result:
                    total_vram += scene_result["avg_vram"]
                    self.results["performance_results"]["peak_vram_usage"] = max(
                        self.results["performance_results"]["peak_vram_usage"],
                        scene_result["peak_vram"]
                    )
                
                if "avg_cpu" in scene_result:
                    self.results["performance_results"]["cpu_usage"] = max(
                        self.results["performance_results"]["cpu_usage"],
                        scene_result["avg_cpu"]
                    )
        
        # è®¡ç®—å¹³å‡æ•°æ®
        if scene_count > 0:
            self.results["performance_results"]["average_fps"] = total_fps / scene_count
            if total_vram > 0:
                self.results["performance_results"]["average_vram_usage"] = total_vram / scene_count
        
        # è¯„ä¼°æ€§èƒ½æµ‹è¯•æ˜¯å¦é€šè¿‡
        self._evaluate_performance_results()
        
        print("\n========== æ€§èƒ½æµ‹è¯•å®Œæˆ ==========")
    
    def _create_mock_scene_result(self, scene_name):
        """åˆ›å»ºæ¨¡æ‹Ÿçš„åœºæ™¯æµ‹è¯•ç»“æœï¼ˆå½“æ— æ³•å®é™…è¿è¡Œæ—¶ä½¿ç”¨ï¼‰
        
        Args:
            scene_name: åœºæ™¯åç§°
            
        Returns:
            dict: æ¨¡æ‹Ÿçš„æµ‹è¯•ç»“æœ
        """
        print("æ³¨æ„: ä½¿ç”¨æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®è¿›è¡Œæµ‹è¯•")
        
        # æ ¹æ®GPUç±»å‹è°ƒæ•´æ¨¡æ‹Ÿæ•°æ®
        gpu_type = "GTX_750Ti"
        if "RX 580" in self.platform_info.gpu_info.upper():
            gpu_type = "RX_580"
        
        # ä¸ºä¸åŒåœºæ™¯è®¾ç½®ä¸åŒçš„æ¨¡æ‹Ÿæ€§èƒ½æ•°æ®
        scene_complexity = {
            "standard_scene": 1.0,
            "high_detail_scene": 1.5,
            "low_light_scene": 1.2,
            "motion_scene": 1.3
        }
        
        complexity = scene_complexity.get(scene_name, 1.0)
        
        # æ ¹æ®GPUç±»å‹å’Œåœºæ™¯å¤æ‚åº¦ç”Ÿæˆæ¨¡æ‹ŸFPS
        if gpu_type == "GTX_750Ti":
            base_fps = 45.0
            base_vram = 1200.0
        else:  # RX 580
            base_fps = 85.0
            base_vram = 2000.0
        
        avg_fps = base_fps / complexity
        
        return {
            "scene_name": scene_name,
            "duration": self.test_config["performance_tests"]["duration_per_test"],
            "avg_fps": avg_fps,
            "min_fps": avg_fps * 0.8,
            "max_fps": avg_fps * 1.1,
            "fps_stability": 0.9,  # å‡è®¾çš„ç¨³å®šæ€§æŒ‡æ ‡
            "avg_vram": base_vram * complexity,
            "peak_vram": base_vram * complexity * 1.1,
            "avg_cpu": 30.0 + (complexity * 5.0),  # æ¨¡æ‹ŸCPUä½¿ç”¨ç‡
            "timestamp": time.time(),
            "is_mock_data": True
        }
    
    def _evaluate_performance_results(self):
        """è¯„ä¼°æ€§èƒ½æµ‹è¯•ç»“æœæ˜¯å¦é€šè¿‡"""
        # ç¡®å®šç›®æ ‡GPUç±»å‹
        gpu_type = "GTX_750Ti"
        if "RX 580" in self.platform_info.gpu_info.upper():
            gpu_type = "RX_580"
        
        # è·å–ç›®æ ‡æ€§èƒ½æŒ‡æ ‡
        target = self.test_config["target_hardware"].get(gpu_type, 
                                                       self.test_config["target_hardware"]["GTX_750Ti"])
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°é¢„æœŸFPS
        fps_passed = self.results["performance_results"]["average_fps"] >= target["expected_fps"]
        
        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡VRAMé™åˆ¶
        vram_passed = self.results["performance_results"]["peak_vram_usage"] <= target["max_vram_usage"]
        
        # æ£€æŸ¥æ¯ä¸ªåœºæ™¯çš„FPSæ˜¯å¦å¯æ¥å—ï¼ˆä¸ä½äºç›®æ ‡çš„70%ï¼‰
        scene_passes = 0
        scene_count = 0
        
        for scene_name, result in self.results["performance_results"]["test_scenes"].items():
            scene_count += 1
            # æ¯ä¸ªåœºæ™¯è‡³å°‘è¾¾åˆ°ç›®æ ‡FPSçš„70%
            if result["avg_fps"] >= target["expected_fps"] * 0.7:
                scene_passes += 1
        
        # è®¡ç®—åœºæ™¯é€šè¿‡ç‡
        scene_pass_rate = scene_passes / scene_count if scene_count > 0 else 0
        scenes_passed = scene_pass_rate >= target["pass_threshold"]
        
        # ç»¼åˆåˆ¤æ–­æ€§èƒ½æµ‹è¯•æ˜¯å¦é€šè¿‡
        self.results["performance_results"]["passed"] = fps_passed and vram_passed and scenes_passed
        
        # ä¿å­˜è¯„ä¼°è¯¦æƒ…
        self.results["performance_results"]["evaluation"] = {
            "target_gpu": gpu_type,
            "target_fps": target["expected_fps"],
            "actual_fps": self.results["performance_results"]["average_fps"],
            "fps_passed": fps_passed,
            "target_max_vram": target["max_vram_usage"],
            "actual_peak_vram": self.results["performance_results"]["peak_vram_usage"],
            "vram_passed": vram_passed,
            "scene_pass_rate": scene_pass_rate,
            "target_scene_pass_rate": target["pass_threshold"],
            "scenes_passed": scenes_passed
        }
    
    def run_quality_tests(self):
        """è¿è¡Œè§†è§‰è´¨é‡éªŒè¯æµ‹è¯•"""
        print("\n========== å¼€å§‹è§†è§‰è´¨é‡æµ‹è¯• ==========")
        
        # åˆå§‹åŒ–è´¨é‡æµ‹è¯•ç»“æœ
        self.results["quality_results"] = {
            "test_scenes": {},
            "average_quality_ratio": 0.0,
            "best_scene": None,
            "worst_scene": None,
            "passed": False
        }
        
        total_quality_ratio = 0.0
        scene_count = 0
        best_quality = 0.0
        worst_quality = 1.0
        
        # æµ‹è¯•åœºæ™¯åˆ—è¡¨
        test_scenes = self.test_config["performance_tests"]["test_scenes"]
        
        for scene_name in test_scenes:
            print(f"\næµ‹è¯•åœºæ™¯: {scene_name}")
            
            # æ„å»ºæ–‡ä»¶è·¯å¾„
            reference_path = os.path.join(
                self.test_config["quality_tests"]["reference_images_dir"],
                f"{scene_name}.png"
            )
            
            rtx_reference_path = os.path.join(
                self.test_config["quality_tests"]["rtx_reference_dir"],
                f"{scene_name}.png"
            )
            
            output_path = os.path.join(
                self.test_config["quality_tests"]["output_images_dir"],
                f"{scene_name}.png"
            )
            
            # å¦‚æœæœ‰å¼•æ“å®ä¾‹ï¼Œæ¸²æŸ“åœºæ™¯
            if self.engine and hasattr(self.engine, 'render'):
                try:
                    print(f"æ¸²æŸ“åœºæ™¯ {scene_name}...")
                    # è¿™é‡Œå‡è®¾å¼•æ“æœ‰renderæ–¹æ³•ï¼Œå®é™…ä½¿ç”¨æ—¶å¯èƒ½éœ€è¦è°ƒæ•´
                    self.engine.render(output_path)
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•æ¸²æŸ“åœºæ™¯ {scene_name}: {e}")
                    # ä½¿ç”¨å‚è€ƒå›¾åƒä½œä¸ºæµ‹è¯•å›¾åƒï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
                    if os.path.exists(reference_path):
                        output_path = reference_path
                    else:
                        print(f"é”™è¯¯: æ‰¾ä¸åˆ°å‚è€ƒå›¾åƒ {reference_path}")
                        continue
            else:
                # æ²¡æœ‰å¼•æ“å®ä¾‹ï¼Œå°è¯•ä½¿ç”¨å‚è€ƒå›¾åƒä½œä¸ºæµ‹è¯•å›¾åƒï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰
                if os.path.exists(reference_path):
                    output_path = reference_path
                    print("æ³¨æ„: ä½¿ç”¨å‚è€ƒå›¾åƒä½œä¸ºæµ‹è¯•å›¾åƒï¼ˆä»…ç”¨äºæ¼”ç¤ºï¼‰")
                else:
                    print(f"é”™è¯¯: æ‰¾ä¸åˆ°å‚è€ƒå›¾åƒ {reference_path}")
                    continue
            
            # æ£€æŸ¥å‚è€ƒå›¾åƒæ˜¯å¦å­˜åœ¨
            if not os.path.exists(reference_path):
                print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å‚è€ƒå›¾åƒ {reference_path}ï¼Œè·³è¿‡è´¨é‡éªŒè¯")
                continue
            
            # æ‰§è¡ŒåŸºæœ¬è´¨é‡éªŒè¯
            print(f"éªŒè¯æ¸²æŸ“è´¨é‡ (å‚è€ƒå›¾åƒ: {reference_path})...")
            basic_validation = self.quality_validator.validate_images(
                reference_path=reference_path,
                test_path=output_path
            )
            
            # ç”Ÿæˆæ¯”è¾ƒå›¾åƒ
            if self.test_config["quality_tests"]["generate_comparison_images"]:
                comparison_path = os.path.join(
                    self.test_config["quality_tests"]["output_images_dir"],
                    f"{scene_name}_comparison.png"
                )
                self.quality_validator.create_comparison_image(comparison_path)
            
            # ç”Ÿæˆé”™è¯¯å›¾
            if self.test_config["quality_tests"]["save_error_maps"]:
                error_map_path = os.path.join(
                    self.test_config["quality_tests"]["output_images_dir"],
                    f"{scene_name}_error_map.png"
                )
                self.quality_validator.generate_error_map(error_map_path)
            
            # ä¸RTX 4090å‚è€ƒè¿›è¡Œæ¯”è¾ƒï¼ˆå¦‚æœæœ‰ï¼‰
            rtx_comparison = None
            if os.path.exists(rtx_reference_path):
                print(f"ä¸RTX 4090å‚è€ƒå›¾åƒæ¯”è¾ƒ (å‚è€ƒå›¾åƒ: {rtx_reference_path})...")
                try:
                    rtx_comparison = self.quality_validator.compare_with_rtx4090_quality(
                        test_image_path=output_path,
                        reference_rtx_path=rtx_reference_path
                    )
                except Exception as e:
                    print(f"è­¦å‘Š: RTX 4090æ¯”è¾ƒå¤±è´¥: {e}")
            
            # ä¿å­˜åœºæ™¯è´¨é‡æµ‹è¯•ç»“æœ
            scene_result = {
                "scene_name": scene_name,
                "reference_image": reference_path,
                "test_image": output_path,
                "basic_validation": basic_validation,
                "rtx_comparison": rtx_comparison,
                "quality_passed": basic_validation["quality_passed"] if basic_validation else False,
                "timestamp": time.time()
            }
            
            self.results["quality_results"]["test_scenes"][scene_name] = scene_result
            
            # æ›´æ–°ç»Ÿè®¡æ•°æ®
            if rtx_comparison and "rtx4090_comparison" in rtx_comparison:
                quality_ratio = rtx_comparison["rtx4090_comparison"]["quality_ratio"]
                total_quality_ratio += quality_ratio
                scene_count += 1
                
                # æ›´æ–°æœ€ä½³å’Œæœ€å·®åœºæ™¯
                if quality_ratio > best_quality:
                    best_quality = quality_ratio
                    self.results["quality_results"]["best_scene"] = scene_name
                
                if quality_ratio < worst_quality:
                    worst_quality = quality_ratio
                    self.results["quality_results"]["worst_scene"] = scene_name
        
        # è®¡ç®—å¹³å‡è´¨é‡æ¯”ç‡
        if scene_count > 0:
            self.results["quality_results"]["average_quality_ratio"] = total_quality_ratio / scene_count
        
        # è¯„ä¼°è´¨é‡æµ‹è¯•æ˜¯å¦é€šè¿‡
        self._evaluate_quality_results()
        
        print("\n========== è§†è§‰è´¨é‡æµ‹è¯•å®Œæˆ ==========")
    
    def _evaluate_quality_results(self):
        """è¯„ä¼°è´¨é‡æµ‹è¯•ç»“æœæ˜¯å¦é€šè¿‡"""
        # ç¡®å®šç›®æ ‡GPUç±»å‹
        gpu_type = "GTX_750Ti"
        if "RX 580" in self.platform_info.gpu_info.upper():
            gpu_type = "RX_580"
        
        # å¯¹äºGTX 750Tiï¼Œæˆ‘ä»¬å¸Œæœ›è‡³å°‘è¾¾åˆ°RTX 4090è´¨é‡çš„75%
        # å¯¹äºRX 580ï¼Œæˆ‘ä»¬å¸Œæœ›è‡³å°‘è¾¾åˆ°RTX 4090è´¨é‡çš„85%
        quality_threshold = 0.75
        if gpu_type == "RX_580":
            quality_threshold = 0.85
        
        # æ£€æŸ¥å¹³å‡è´¨é‡æ¯”ç‡æ˜¯å¦è¾¾æ ‡
        quality_ratio_passed = self.results["quality_results"]["average_quality_ratio"] >= quality_threshold
        
        # æ£€æŸ¥æ¯ä¸ªåœºæ™¯çš„åŸºæœ¬è´¨é‡éªŒè¯æ˜¯å¦é€šè¿‡
        scene_passes = 0
        scene_count = 0
        
        for scene_name, result in self.results["quality_results"]["test_scenes"].items():
            scene_count += 1
            if result["quality_passed"]:
                scene_passes += 1
        
        # è®¡ç®—åœºæ™¯é€šè¿‡ç‡
        scene_pass_rate = scene_passes / scene_count if scene_count > 0 else 0
        
        # å¯¹äºè´¨é‡æµ‹è¯•ï¼Œæˆ‘ä»¬è¦æ±‚æ‰€æœ‰åœºæ™¯éƒ½é€šè¿‡åŸºæœ¬è´¨é‡éªŒè¯
        scenes_passed = scene_pass_rate >= 1.0
        
        # ç»¼åˆåˆ¤æ–­è´¨é‡æµ‹è¯•æ˜¯å¦é€šè¿‡
        self.results["quality_results"]["passed"] = quality_ratio_passed and scenes_passed
        
        # ä¿å­˜è¯„ä¼°è¯¦æƒ…
        self.results["quality_results"]["evaluation"] = {
            "target_gpu": gpu_type,
            "target_quality_ratio": quality_threshold,
            "actual_quality_ratio": self.results["quality_results"]["average_quality_ratio"],
            "quality_ratio_passed": quality_ratio_passed,
            "scene_pass_rate": scene_pass_rate,
            "scenes_passed": scenes_passed
        }
    
    def _generate_summary(self):
        """ç”Ÿæˆç»¼åˆæµ‹è¯•ç»“æœæ‘˜è¦"""
        # è®¡ç®—æ€»æµ‹è¯•æ•°å’Œé€šè¿‡æ•°
        total_tests = 0
        passed_tests = 0
        
        # æ€§èƒ½æµ‹è¯•ç»Ÿè®¡
        if self.test_config["performance_tests"]["enabled"]:
            total_tests += 1
            if self.results["performance_results"]["passed"]:
                passed_tests += 1
        
        # è´¨é‡æµ‹è¯•ç»Ÿè®¡
        if self.test_config["quality_tests"]["enabled"]:
            total_tests += 1
            if self.results["quality_results"]["passed"]:
                passed_tests += 1
        
        # è®¡ç®—é€šè¿‡ç‡
        pass_rate = passed_tests / total_tests if total_tests > 0 else 0
        
        # ä¿å­˜æ‘˜è¦ä¿¡æ¯
        self.results["summary"]["total_tests"] = total_tests
        self.results["summary"]["passed_tests"] = passed_tests
        self.results["summary"]["pass_rate"] = pass_rate
        
        # æ•´ä½“æ˜¯å¦é€šè¿‡
        self.results["summary"]["performance_passed"] = (
            self.results["performance_results"]["passed"] 
            if self.test_config["performance_tests"]["enabled"] else False
        )
        
        self.results["summary"]["quality_passed"] = (
            self.results["quality_results"]["passed"] 
            if self.test_config["quality_tests"]["enabled"] else False
        )
        
        # æ•´ä½“æµ‹è¯•é€šè¿‡æ¡ä»¶ï¼šæ‰€æœ‰å¯ç”¨çš„æµ‹è¯•éƒ½é€šè¿‡
        performance_ok = not self.test_config["performance_tests"]["enabled"] or self.results["summary"]["performance_passed"]
        quality_ok = not self.test_config["quality_tests"]["enabled"] or self.results["summary"]["quality_passed"]
        
        self.results["summary"]["overall_passed"] = performance_ok and quality_ok
        
        # æ·»åŠ ä¼˜åŒ–å»ºè®®
        self.results["summary"]["optimization_suggestions"] = self._generate_optimization_suggestions()
    
    def _generate_optimization_suggestions(self):
        """åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆä¼˜åŒ–å»ºè®®
        
        Returns:
            list: ä¼˜åŒ–å»ºè®®åˆ—è¡¨
        """
        suggestions = []
        
        # æ€§èƒ½ä¼˜åŒ–å»ºè®®
        if self.test_config["performance_tests"]["enabled"]:
            perf_results = self.results["performance_results"]
            perf_eval = perf_results["evaluation"]
            
            # FPSä¼˜åŒ–å»ºè®®
            if not perf_eval["fps_passed"]:
                fps_deficit = perf_eval["target_fps"] - perf_eval["actual_fps"]
                percentage = (fps_deficit / perf_eval["target_fps"]) * 100
                
                if percentage > 30:
                    suggestions.append("æ€§èƒ½ä¸¥é‡ä¸è¶³ï¼Œå»ºè®®ï¼š")
                    suggestions.append("  - é™ä½æ¸²æŸ“åˆ†è¾¨ç‡")
                    suggestions.append("  - å‡å°‘è§†å£ä¸­çš„å®ä½“æ•°é‡")
                    suggestions.append("  - å…³é—­å¤æ‚çš„åå¤„ç†æ•ˆæœ")
                elif percentage > 15:
                    suggestions.append("æ€§èƒ½ç•¥ä½äºç›®æ ‡ï¼Œå»ºè®®ï¼š")
                    suggestions.append("  - é™ä½çº¹ç†åˆ†è¾¨ç‡æˆ–ä½¿ç”¨æ›´æ¿€è¿›çš„å‹ç¼©")
                    suggestions.append("  - ä¼˜åŒ–å‡ ä½•ä½“ï¼Œå¢åŠ LODçº§åˆ«")
                    suggestions.append("  - å‡å°‘é˜´å½±åˆ†è¾¨ç‡æˆ–å¤æ‚åº¦")
                else:
                    suggestions.append("æ€§èƒ½æ¥è¿‘ç›®æ ‡ï¼Œå»ºè®®å¾®è°ƒï¼š")
                    suggestions.append("  - ä¼˜åŒ–ç€è‰²å™¨å¤æ‚åº¦")
                    suggestions.append("  - æ£€æŸ¥æ˜¯å¦æœ‰CPUç“¶é¢ˆ")
            
            # VRAMä¼˜åŒ–å»ºè®®
            if not perf_eval["vram_passed"]:
                vram_over = perf_eval["actual_peak_vram"] - perf_eval["target_max_vram"]
                suggestions.append(f"æ˜¾å­˜ä½¿ç”¨è¶…å‡ºé™åˆ¶çº¦ {vram_over:.1f} MBï¼Œå»ºè®®ï¼š")
                suggestions.append("  - ä½¿ç”¨æ›´æ¿€è¿›çš„çº¹ç†å‹ç¼©")
                suggestions.append("  - å®ç°æ›´é«˜æ•ˆçš„çº¹ç†æµå¼åŠ è½½")
                suggestions.append("  - ä¼˜åŒ–å‡ ä½•ä½“ä»¥å‡å°‘é¡¶ç‚¹æ•°æ®")
            
            # ç‰¹å®šåœºæ™¯æ€§èƒ½é—®é¢˜
            if not perf_eval["scenes_passed"]:
                suggestions.append("éƒ¨åˆ†åœºæ™¯æ€§èƒ½ä¸è¾¾æ ‡ï¼Œå»ºè®®é’ˆå¯¹ç‰¹å®šåœºæ™¯è¿›è¡Œä¼˜åŒ–ï¼š")
                
                # æ‰¾å‡ºæ€§èƒ½æœ€å·®çš„åœºæ™¯
                worst_scene = None
                worst_fps = float('inf')
                
                for scene_name, result in perf_results["test_scenes"].items():
                    if result["avg_fps"] < worst_fps:
                        worst_fps = result["avg_fps"]
                        worst_scene = scene_name
                
                if worst_scene:
                    suggestions.append(f"  - ä¼˜å…ˆä¼˜åŒ– '{worst_scene}' åœºæ™¯")
                    suggestions.append(f"  - å½“å‰FPS: {worst_fps:.1f}ï¼Œç›®æ ‡FPS: {perf_eval['target_fps']*0.7:.1f}")
        
        # è´¨é‡ä¼˜åŒ–å»ºè®®
        if self.test_config["quality_tests"]["enabled"]:
            qual_results = self.results["quality_results"]
            qual_eval = qual_results["evaluation"]
            
            # è´¨é‡æ¯”ç‡ä¼˜åŒ–å»ºè®®
            if not qual_eval["quality_ratio_passed"]:
                quality_deficit = qual_eval["target_quality_ratio"] - qual_eval["actual_quality_ratio"]
                percentage = (quality_deficit / qual_eval["target_quality_ratio"]) * 100
                
                suggestions.append(f"è§†è§‰è´¨é‡ä¸RTX 4090å‚è€ƒç›¸æ¯”å·®è·è¾ƒå¤§ ({percentage:.1f}%)ï¼Œå»ºè®®ï¼š")
                
                # æ£€æŸ¥æœ€å·®åœºæ™¯
                if qual_results["worst_scene"]:
                    worst_scene = qual_results["worst_scene"]
                    scene_data = qual_results["test_scenes"].get(worst_scene, {})
                    
                    if scene_data and "rtx_comparison" in scene_data and scene_data["rtx_comparison"]:
                        rtx_comp = scene_data["rtx_comparison"]["rtx4090_comparison"]
                        key_diffs = rtx_comp.get("key_differences", [])
                        
                        if key_diffs:
                            suggestions.append(f"  '{worst_scene}' åœºæ™¯çš„ä¸»è¦é—®é¢˜ï¼š")
                            for diff in key_diffs[:2]:  # åªæ˜¾ç¤ºå‰ä¸¤ä¸ªä¸»è¦é—®é¢˜
                                suggestions.append(f"    - {diff}")
            
            # åœºæ™¯è´¨é‡éªŒè¯å»ºè®®
            if not qual_eval["scenes_passed"]:
                suggestions.append("éƒ¨åˆ†åœºæ™¯æœªé€šè¿‡åŸºæœ¬è´¨é‡éªŒè¯ï¼Œå»ºè®®ï¼š")
                suggestions.append("  - æ£€æŸ¥æ¸²æŸ“ç®¡çº¿ä¸­çš„é¢œè‰²å¤„ç†å’Œå…‰ç…§è®¡ç®—")
                suggestions.append("  - éªŒè¯çº¹ç†æ˜ å°„å’Œæè´¨å‚æ•°æ˜¯å¦æ­£ç¡®")
                suggestions.append("  - æ£€æŸ¥æ˜¯å¦æœ‰å‡ ä½•æ¸²æŸ“é”™è¯¯")
        
        # å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œæä¾›ä¸€äº›è¿›ä¸€æ­¥ä¼˜åŒ–çš„å»ºè®®
        if self.results["summary"]["overall_passed"]:
            suggestions.append("\næ­å–œï¼æ‰€æœ‰æµ‹è¯•å‡å·²é€šè¿‡ã€‚è¿›ä¸€æ­¥ä¼˜åŒ–å»ºè®®ï¼š")
            
            # æ€§èƒ½ä¼˜åŒ–ç©ºé—´
            if self.test_config["performance_tests"]["enabled"]:
                perf_eval = self.results["performance_results"]["evaluation"]
                fps_headroom = (perf_eval["actual_fps"] - perf_eval["target_fps"]) / perf_eval["target_fps"] * 100
                
                if fps_headroom > 30:
                    suggestions.append("  - æ€§èƒ½æœ‰è¾ƒå¤§ä½™é‡ï¼Œå¯ä»¥è€ƒè™‘æå‡è§†è§‰è´¨é‡")
                    suggestions.append("  - å°è¯•å¢åŠ ä¸€äº›é«˜çº§æ¸²æŸ“æ•ˆæœï¼Œå¦‚æ›´å¤æ‚çš„å…‰ç…§æˆ–åå¤„ç†")
                
                # VRAMä¼˜åŒ–ç©ºé—´
                vram_headroom = perf_eval["target_max_vram"] - perf_eval["actual_peak_vram"]
                if vram_headroom > 500:
                    suggestions.append(f"  - æ˜¾å­˜ä½¿ç”¨æœ‰ {vram_headroom:.1f} MB çš„ä½™é‡ï¼Œå¯ä»¥è€ƒè™‘ï¼š")
                    suggestions.append("    * æé«˜å…³é”®çº¹ç†çš„åˆ†è¾¨ç‡")
                    suggestions.append("    * å¢åŠ çº¹ç†ç»†èŠ‚æˆ–å‡å°‘å‹ç¼©")
            
            # è´¨é‡æå‡ç©ºé—´
            if self.test_config["quality_tests"]["enabled"]:
                qual_eval = self.results["quality_results"]["evaluation"]
                quality_gap = 1.0 - qual_eval["actual_quality_ratio"]
                
                if quality_gap > 0:
                    suggestions.append(f"  - ä¸RTX 4090ç›¸æ¯”ï¼Œè¿˜æœ‰ {quality_gap*100:.1f}% çš„è´¨é‡æå‡ç©ºé—´")
                    suggestions.append("  - å¯ä»¥è€ƒè™‘å®ç°æ›´é«˜çº§çš„å…‰ç…§ç®—æ³•æˆ–åå¤„ç†æ•ˆæœ")
        
        return suggestions
    
    def _save_reports(self):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        # åˆ›å»ºæŠ¥å‘Šç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        reports_dir = self.test_config["output"]["reports_dir"]
        os.makedirs(reports_dir, exist_ok=True)
        
        # ç”Ÿæˆæ—¶é—´æˆ³ç”¨äºæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜JSONæŠ¥å‘Š
        if self.test_config["output"]["save_json_report"]:
            json_filename = os.path.join(reports_dir, f"test_report_{timestamp}.json")
            try:
                with open(json_filename, 'w', encoding='utf-8') as f:
                    json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)
                print(f"JSONæŠ¥å‘Šå·²ä¿å­˜è‡³: {json_filename}")
            except Exception as e:
                print(f"ä¿å­˜JSONæŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        if self.test_config["output"]["save_text_report"]:
            txt_filename = os.path.join(reports_dir, f"test_report_{timestamp}.txt")
            try:
                with open(txt_filename, 'w', encoding='utf-8') as f:
                    self._write_text_report(f)
                print(f"æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜è‡³: {txt_filename}")
            except Exception as e:
                print(f"ä¿å­˜æ–‡æœ¬æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
    
    def _write_text_report(self, file):
        """å°†æ–‡æœ¬æŠ¥å‘Šå†™å…¥æ–‡ä»¶
        
        Args:
            file: æ–‡ä»¶å¯¹è±¡
        """
        # æŠ¥å‘Šæ ‡é¢˜
        file.write("="*60 + "\n")
        file.write("          ä½ç«¯GPUæ¸²æŸ“å¼•æ“ç»¼åˆæµ‹è¯•æŠ¥å‘Š          \n")
        file.write("="*60 + "\n\n")
        
        # æµ‹è¯•ä¿¡æ¯
        file.write(f"æµ‹è¯•æ—¶é—´: {self.results['test_date']}\n\n")
        
        # å¹³å°ä¿¡æ¯
        file.write("å¹³å°ä¿¡æ¯:\n")
        file.write("-"*30 + "\n")
        platform_info = self.results["platform_info"]
        file.write(f"æ“ä½œç³»ç»Ÿ: {platform_info['os']}\n")
        file.write(f"GPU: {platform_info['gpu']}\n")
        file.write(f"VRAM: {platform_info['gpu_memory']} MB\n")
        file.write(f"CPU: {platform_info['cpu']}\n")
        file.write(f"ç³»ç»Ÿå†…å­˜: {platform_info['system_memory']} GB\n")
        file.write(f"DirectXç‰ˆæœ¬: {platform_info['directx_version']}\n")
        file.write(f"OpenGLç‰ˆæœ¬: {platform_info['opengl_version']}\n\n")
        
        # æ€§èƒ½æµ‹è¯•ç»“æœ
        if self.test_config["performance_tests"]["enabled"]:
            perf_results = self.results["performance_results"]
            perf_eval = perf_results["evaluation"]
            
            file.write("æ€§èƒ½æµ‹è¯•ç»“æœ:\n")
            file.write("-"*30 + "\n")
            file.write(f"æµ‹è¯•çŠ¶æ€: {'é€šè¿‡' if perf_results['passed'] else 'æœªé€šè¿‡'}\n")
            file.write(f"ç›®æ ‡GPUç±»å‹: {perf_eval['target_gpu']}\n")
            file.write(f"å¹³å‡FPS: {perf_results['average_fps']:.1f} (ç›®æ ‡: {perf_eval['target_fps']})\n")
            file.write(f"æœ€ä½FPS: {perf_results['min_fps']:.1f}\n")
            file.write(f"æœ€é«˜FPS: {perf_results['max_fps']:.1f}\n")
            file.write(f"å¹³å‡æ˜¾å­˜ä½¿ç”¨: {perf_results['average_vram_usage']:.1f} MB\n")
            file.write(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {perf_results['peak_vram_usage']:.1f} MB (é™åˆ¶: {perf_eval['target_max_vram']} MB)\n")
            file.write(f"å¹³å‡CPUä½¿ç”¨ç‡: {perf_results['cpu_usage']:.1f}%\n")
            file.write(f"åœºæ™¯é€šè¿‡ç‡: {perf_eval['scene_pass_rate']*100:.1f}% (ç›®æ ‡: {perf_eval['target_scene_pass_rate']*100:.0f}%)\n\n")
            
            # å„åœºæ™¯æ€§èƒ½è¯¦æƒ…
            file.write("å„åœºæ™¯æ€§èƒ½è¯¦æƒ…:\n")
            for scene_name, result in perf_results["test_scenes"].items():
                status = "âœ“" if result["avg_fps"] >= perf_eval["target_fps"] * 0.7 else "âœ—"
                file.write(f"  {status} {scene_name}: {result['avg_fps']:.1f} FPS, VRAM: {result['peak_vram']:.1f} MB\n")
            file.write("\n")
        
        # è´¨é‡æµ‹è¯•ç»“æœ
        if self.test_config["quality_tests"]["enabled"]:
            qual_results = self.results["quality_results"]
            qual_eval = qual_results["evaluation"]
            
            file.write("è§†è§‰è´¨é‡æµ‹è¯•ç»“æœ:\n")
            file.write("-"*30 + "\n")
            file.write(f"æµ‹è¯•çŠ¶æ€: {'é€šè¿‡' if qual_results['passed'] else 'æœªé€šè¿‡'}\n")
            file.write(f"ç›®æ ‡GPUç±»å‹: {qual_eval['target_gpu']}\n")
            file.write(f"å¹³å‡RTX 4090è´¨é‡ç›¸ä¼¼åº¦: {qual_results['average_quality_ratio']*100:.1f}% (ç›®æ ‡: {qual_eval['target_quality_ratio']*100:.0f}%)\n")
            file.write(f"æœ€ä½³åœºæ™¯: {qual_results['best_scene']}\n")
            file.write(f"æœ€å·®åœºæ™¯: {qual_results['worst_scene']}\n")
            file.write(f"åœºæ™¯è´¨é‡é€šè¿‡ç‡: {qual_eval['scene_pass_rate']*100:.1f}%\n\n")
            
            # å„åœºæ™¯è´¨é‡è¯¦æƒ…
            file.write("å„åœºæ™¯è´¨é‡è¯¦æƒ…:\n")
            for scene_name, result in qual_results["test_scenes"].items():
                status = "âœ“" if result["quality_passed"] else "âœ—"
                rtx_ratio = "N/A"
                if result.get("rtx_comparison") and "rtx4090_comparison" in result["rtx_comparison"]:
                    rtx_ratio = f"{result['rtx_comparison']['rtx4090_comparison']['quality_ratio']*100:.1f}%"
                file.write(f"  {status} {scene_name}: è´¨é‡éªŒè¯{'é€šè¿‡' if result['quality_passed'] else 'æœªé€šè¿‡'}, RTXç›¸ä¼¼åº¦: {rtx_ratio}\n")
            file.write("\n")
        
        # ç»¼åˆç»“è®º
        file.write("ç»¼åˆç»“è®º:\n")
        file.write("-"*30 + "\n")
        summary = self.results["summary"]
        
        if summary["overall_passed"]:
            file.write("ğŸ‰ æ­å–œï¼æ‰€æœ‰æµ‹è¯•å‡å·²é€šè¿‡ï¼\n")
        else:
            file.write("âŒ æµ‹è¯•æœªå…¨éƒ¨é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚\n")
        
        file.write(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}\n")
        file.write(f"é€šè¿‡æµ‹è¯•æ•°: {summary['passed_tests']}\n")
        file.write(f"é€šè¿‡ç‡: {summary['pass_rate']*100:.1f}%\n\n")
        
        # ä¼˜åŒ–å»ºè®®
        file.write("ä¼˜åŒ–å»ºè®®:\n")
        file.write("-"*30 + "\n")
        for suggestion in summary["optimization_suggestions"]:
            file.write(f"{suggestion}\n")
        
        if not summary["optimization_suggestions"]:
            file.write("æ— å…·ä½“ä¼˜åŒ–å»ºè®®ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚\n")
        
        file.write("\n" + "="*60)
    
    def _display_summary(self):
        """æ˜¾ç¤ºæµ‹è¯•ç»“æœæ‘˜è¦"""
        summary = self.results["summary"]
        
        print("\n========== æµ‹è¯•ç»“æœæ‘˜è¦ ==========")
        
        if summary["overall_passed"]:
            print("ğŸ‰ æ­å–œï¼æ‰€æœ‰æµ‹è¯•å‡å·²é€šè¿‡ï¼")
        else:
            print("âŒ æµ‹è¯•æœªå…¨éƒ¨é€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•æ•°: {summary['passed_tests']}")
        print(f"é€šè¿‡ç‡: {summary['pass_rate']*100:.1f}%")
        
        # æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
        if self.test_config["performance_tests"]["enabled"]:
            perf_results = self.results["performance_results"]
            print(f"\næ€§èƒ½æµ‹è¯•: {'é€šè¿‡' if summary['performance_passed'] else 'æœªé€šè¿‡'}")
            print(f"  å¹³å‡FPS: {perf_results['average_fps']:.1f}")
            print(f"  å³°å€¼æ˜¾å­˜ä½¿ç”¨: {perf_results['peak_vram_usage']:.1f} MB")
        
        # æ˜¾ç¤ºè´¨é‡æ‘˜è¦
        if self.test_config["quality_tests"]["enabled"]:
            qual_results = self.results["quality_results"]
            print(f"\nè§†è§‰è´¨é‡æµ‹è¯•: {'é€šè¿‡' if summary['quality_passed'] else 'æœªé€šè¿‡'}")
            print(f"  RTX 4090è´¨é‡ç›¸ä¼¼åº¦: {qual_results['average_quality_ratio']*100:.1f}%")
            print(f"  æœ€ä½³åœºæ™¯: {qual_results['best_scene']}")
            print(f"  æœ€å·®åœºæ™¯: {qual_results['worst_scene']}")
        
        # æ˜¾ç¤ºå…³é”®ä¼˜åŒ–å»ºè®®
        print("\nå…³é”®ä¼˜åŒ–å»ºè®®:")
        if summary["optimization_suggestions"]:
            # åªæ˜¾ç¤ºå‰5æ¡å»ºè®®
            for i, suggestion in enumerate(summary["optimization_suggestions"][:5]):
                print(f"  {suggestion}")
            
            if len(summary["optimization_suggestions"]) > 5:
                print(f"  ... è¿˜æœ‰ {len(summary["optimization_suggestions"]) - 5} æ¡å»ºè®®ï¼Œè¯·æŸ¥çœ‹å®Œæ•´æŠ¥å‘Š")
        else:
            print("  æ— å…·ä½“ä¼˜åŒ–å»ºè®®ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè¿›è¡Œè°ƒæ•´ã€‚")
    
    def export_test_results_for_analysis(self, filename=None):
        """å¯¼å‡ºæµ‹è¯•ç»“æœç”¨äºè¿›ä¸€æ­¥åˆ†æ
        
        Args:
            filename: å¯¼å‡ºæ–‡ä»¶åï¼Œå¦‚ä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: å¯¼å‡ºæ–‡ä»¶è·¯å¾„
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = os.path.join(
                self.test_config["output"]["reports_dir"],
                f"test_results_analysis_{timestamp}.json"
            )
        
        # å‡†å¤‡åˆ†ææ•°æ®
        analysis_data = {
            "test_date": self.results["test_date"],
            "platform_info": self.results["platform_info"],
            "performance_metrics": {},
            "quality_metrics": {},
            "comparison_metrics": {}
        }
        
        # æå–æ€§èƒ½æŒ‡æ ‡
        if self.test_config["performance_tests"]["enabled"]:
            perf_results = self.results["performance_results"]
            analysis_data["performance_metrics"] = {
                "scenes": {},
                "averages": {
                    "fps": perf_results["average_fps"],
                    "vram_usage": perf_results["average_vram_usage"],
                    "cpu_usage": perf_results["cpu_usage"]
                }
            }
            
            # ä¸ºæ¯ä¸ªåœºæ™¯æå–è¯¦ç»†æŒ‡æ ‡
            for scene_name, result in perf_results["test_scenes"].items():
                analysis_data["performance_metrics"]["scenes"][scene_name] = {
                    "fps": {
                        "avg": result["avg_fps"],
                        "min": result["min_fps"],
                        "max": result["max_fps"]
                    },
                    "vram": {
                        "avg": result.get("avg_vram", 0),
                        "peak": result.get("peak_vram", 0)
                    },
                    "cpu": {
                        "avg": result.get("avg_cpu", 0)
                    }
                }
        
        # æå–è´¨é‡æŒ‡æ ‡
        if self.test_config["quality_tests"]["enabled"]:
            qual_results = self.results["quality_results"]
            analysis_data["quality_metrics"] = {
                "scenes": {},
                "average_quality_ratio": qual_results["average_quality_ratio"]
            }
            
            # ä¸ºæ¯ä¸ªåœºæ™¯æå–è¯¦ç»†è´¨é‡æŒ‡æ ‡
            for scene_name, result in qual_results["test_scenes"].items():
                scene_metrics = {
                    "quality_passed": result["quality_passed"]
                }
                
                # æ·»åŠ åŸºæœ¬éªŒè¯æŒ‡æ ‡
                if result.get("basic_validation") and "metrics" in result["basic_validation"]:
                    metrics = result["basic_validation"]["metrics"]
                    scene_metrics["basic_metrics"] = {
                        "psnr": metrics.get("psnr", 0),
                        "ssim": metrics.get("ssim", 0),
                        "mse": metrics.get("mse", 0),
                        "error_pixel_percentage": metrics.get("error_pixel_percentage", 0),
                        "perceptual_similarity": metrics.get("perceptual_similarity", 0)
                    }
                
                # æ·»åŠ RTXæ¯”è¾ƒæŒ‡æ ‡
                if result.get("rtx_comparison") and "rtx4090_comparison" in result["rtx_comparison"]:
                    rtx_comp = result["rtx_comparison"]["rtx4090_comparison"]
                    scene_metrics["rtx_comparison"] = {
                        "quality_ratio": rtx_comp.get("quality_ratio", 0),
                        "visual_fidelity_level": rtx_comp.get("visual_fidelity", {}).get("level", "æœªçŸ¥")
                    }
                
                analysis_data["quality_metrics"]["scenes"][scene_name] = scene_metrics
        
        # è®¡ç®—æ¯”è¾ƒæŒ‡æ ‡
        if self.test_config["performance_tests"]["enabled"] and self.test_config["quality_tests"]["enabled"]:
            perf_results = self.results["performance_results"]
            qual_results = self.results["quality_results"]
            
            # è®¡ç®—æ€§èƒ½-è´¨é‡å¹³è¡¡æŒ‡æ ‡
            # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªç®€å•çš„å…¬å¼ï¼š(fps/ç›®æ ‡fps) * 0.4 + (quality_ratio) * 0.6
            gpu_type = "GTX_750Ti"
            if "RX 580" in self.platform_info.gpu_info.upper():
                gpu_type = "RX_580"
            
            target_fps = self.test_config["target_hardware"].get(gpu_type, 
                                                              self.test_config["target_hardware"]["GTX_750Ti"])["expected_fps"]
            
            fps_ratio = min(perf_results["average_fps"] / target_fps, 1.0)
            quality_ratio = qual_results["average_quality_ratio"]
            
            # æ€§èƒ½-è´¨é‡å¹³è¡¡åˆ†æ•°ï¼ˆ0-100ï¼‰
            balance_score = (fps_ratio * 0.4 + quality_ratio * 0.6) * 100
            
            analysis_data["comparison_metrics"] = {
                "performance_quality_balance": balance_score,
                "fps_efficiency": fps_ratio * 100,  # æ€§èƒ½æ•ˆç‡ç™¾åˆ†æ¯”
                "quality_achievement": quality_ratio * 100,  # è´¨é‡è¾¾æˆç™¾åˆ†æ¯”
                "gpu_type": gpu_type
            }
        
        # ä¿å­˜åˆ†ææ•°æ®
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_data, f, ensure_ascii=False, indent=2, default=str)
            print(f"åˆ†ææ•°æ®å·²å¯¼å‡ºè‡³: {filename}")
            return filename
        except Exception as e:
            print(f"å¯¼å‡ºåˆ†ææ•°æ®æ—¶å‡ºé”™: {e}")
            return None


def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ç»¼åˆæµ‹è¯•å·¥å…·"""
    print("å¯åŠ¨ä½ç«¯GPUæ¸²æŸ“å¼•æ“ç»¼åˆæµ‹è¯•å·¥å…·...")
    
    # åˆ›å»ºæµ‹è¯•å™¨å®ä¾‹
    tester = ComprehensiveTester()
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    results = tester.run_all_tests()
    
    # å¯¼å‡ºåˆ†ææ•°æ®
    tester.export_test_results_for_analysis()
    
    print("æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()